#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import json
import argparse
import subprocess
from datetime import datetime
from typing import List, Dict, Any

import torch

# ---------------------------------------------------------------------
# Í∞ÑÎã® Ïú†Ìã∏
# ---------------------------------------------------------------------
def get_git_commit_or_none():
    try:
        out = subprocess.check_output(
            ["git", "rev-parse", "--short", "HEAD"],
            stderr=subprocess.DEVNULL,
        )
        return out.decode("utf-8").strip()
    except Exception:
        return None


def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)
    return p


def write_json(path: str, obj: dict):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, ensure_ascii=False)


def write_text(path: str, text: str):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)


def touch_latest_symlink(base_dir: str, target_dir: str):
    """
    base_dir/<latest> ‚Üí target_dir Î°ú Ïã¨Î≥ºÎ¶≠ ÎßÅÌÅ¨ Í∞±Ïã† (Í∞ÄÎä•Ìï† ÎïåÎßå).
    """
    latest = os.path.join(base_dir, "latest")
    try:
        if os.path.islink(latest) or os.path.exists(latest):
            try:
                os.remove(latest)
            except IsADirectoryError:
                import shutil
                shutil.rmtree(latest, ignore_errors=True)
        os.symlink(target_dir, latest)
    except Exception as e:
        print(f"[WARN] Could not update latest symlink: {e}")


def parse_gpu_list(arg: str | None) -> List[int]:
    if arg is None or arg == "":
        return []
    if isinstance(arg, int):
        return [arg]
    parts = [p.strip() for p in str(arg).split(",") if p.strip() != ""]
    out = []
    for p in parts:
        try:
            out.append(int(p))
        except Exception:
            pass
    return out


# ---------------------------------------------------------------------
# Îã®Ïùº Ïã§Ìñâ(ÏòàÏ†Ñ Î∞©Ïãù) Î°úÏßÅ
# ---------------------------------------------------------------------
def single_run(
    *,
    mode_key: str,            # "run" | "projection" | "base" | "residual"
    model: str,
    gpu_sel: str,             # "cpu" | "cuda" | "cuda:<idx>"
    plots_base: str,
    tag: str | None = None,
):
    ROOT = os.path.dirname(os.path.abspath(__file__))
    CORE_PATH = os.path.join(ROOT, "core")
    TEST_PATH = os.path.join(ROOT, "tests", model)

    # import Í≤ΩÎ°ú
    sys.path.insert(0, CORE_PATH)
    sys.path.insert(0, TEST_PATH)

    print(f"‚úÖ Running with model: {model}")
    print(f"‚úÖ Core path: {CORE_PATH}")
    print(f"‚úÖ Test path: {TEST_PATH}")

    # GPU ÏÑ†ÌÉù(Îã®Ïùº Î™®Îìú)
    if gpu_sel.startswith("cuda"):
        try:
            # gpu_sel Ïù¥ "cuda" ÎòêÎäî "cuda:<idx>"
            if ":" in gpu_sel:
                idx = int(gpu_sel.split(":")[1])
                torch.cuda.set_device(idx)
                sel = f"cuda:{idx}"
            else:
                sel = "cuda"
        except Exception:
            sel = "cuda"
    else:
        sel = "cpu" if not torch.cuda.is_available() else "cuda"
    print(f"‚úÖ Selected GPU: {sel}")

    # ÏãúÍ∞Ñ/Ï∂úÎ†• Í≤ΩÎ°ú: plots/<model>/<mode>/<timestamp>[_tag]
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    ts_name = ts if tag is None else f"{ts}_{tag}"
    OUT_DIR = os.path.join(ROOT, plots_base, model, mode_key, ts_name)
    ensure_dir(OUT_DIR)
    print(f"‚úÖ Output dir: {OUT_DIR}")

    mode_human = {
        "run": "Variance-Reduced Simulation (Default)",
        "projection": "P-PGDPO with Projection",
        "base": "Basic Simulation",
        "residual": "Residual Learning",
    }[mode_key]
    print(f"üöÄ Mode: {mode_human}")

    # Í≥µÌÜµ Ïã¨Î≥º Î°úÎìú
    from pgdpo_base import (
        seed, epochs, batch_size, lr,
        T, m, d, k,
        CRN_SEED_EU,
    )

    # P-PGDPO Î∞òÎ≥µ ÏÑ§Ï†ï(ÏûàÏúºÎ©¥)
    REPEATS = None
    SUBBATCH = None
    try:
        from pgdpo_with_projection import REPEATS as _R, SUBBATCH as _S
        REPEATS, SUBBATCH = int(_R), int(_S)
    except Exception:
        pass

    # Î©îÌÉÄ Í∏∞Î°ù
    meta = {
        "model": model,
        "mode": mode_key,
        "mode_human": mode_human,
        "timestamp": ts,
        "outdir": OUT_DIR,
        "core_path": CORE_PATH,
        "test_path": TEST_PATH,
        "selected_gpu": sel,
        "torch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "cuda_device_name": torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else None,
        "git_commit": get_git_commit_or_none(),
        "seed": int(seed),
        "epochs": int(epochs),
        "batch_size": int(batch_size),
        "lr": float(lr),
        "T": float(T),
        "m": int(m),
        "d": int(d),
        "k": int(k),
        "CRN_SEED_EU": int(CRN_SEED_EU),
        "REPEATS": REPEATS,
        "SUBBATCH": SUBBATCH,
    }
    write_json(os.path.join(OUT_DIR, "run.json"), meta)
    write_text(
        os.path.join(OUT_DIR, "run.txt"),
        (
            f"Model : {model}\n"
            f"Mode  : {mode_key} ({mode_human})\n"
            f"Time  : {ts}\n"
            f"GPU   : {sel}\n"
            f"Core  : {CORE_PATH}\n"
            f"Test  : {TEST_PATH}\n"
        ),
    )
    # latest ÎßÅÌÅ¨ Í∞±Ïã†
    base_mode_dir = os.path.join(ROOT, plots_base, model, mode_key)
    touch_latest_symlink(base_mode_dir, OUT_DIR)

    # Î™®ÎìúÎ≥Ñ Ïã§Ìñâ
    from pgdpo_base import run_common

    if mode_key == "run":
        from pgdpo_run import train_stage1_run, print_policy_rmse_and_samples_run
        train_fn = train_stage1_run
        rmse_fn = print_policy_rmse_and_samples_run
        train_kwargs = {"outdir": OUT_DIR}
        rmse_kwargs = {"repeats": REPEATS, "sub_batch": SUBBATCH, "seed_eval": CRN_SEED_EU, "outdir": OUT_DIR}

    elif mode_key == "projection":
        from pgdpo_base import train_stage1_base
        from pgdpo_with_projection import print_policy_rmse_and_samples_direct
        train_fn = train_stage1_base
        rmse_fn = print_policy_rmse_and_samples_direct
        train_kwargs = {"outdir": OUT_DIR}
        rmse_kwargs = {"repeats": REPEATS, "sub_batch": SUBBATCH, "seed_eval": CRN_SEED_EU, "outdir": OUT_DIR}

    elif mode_key == "base":
        from pgdpo_base import train_stage1_base, print_policy_rmse_and_samples_base
        train_fn = train_stage1_base
        rmse_fn = print_policy_rmse_and_samples_base
        train_kwargs = {"outdir": OUT_DIR}
        rmse_kwargs = {"repeats": REPEATS, "sub_batch": SUBBATCH, "seed_eval": CRN_SEED_EU, "outdir": OUT_DIR}

    else:  # "residual"
        from pgdpo_residual import train_residual_stage1
        from pgdpo_run import print_policy_rmse_and_samples_run
        train_fn = train_residual_stage1
        rmse_fn = print_policy_rmse_and_samples_run
        train_kwargs = {"outdir": OUT_DIR}
        rmse_kwargs = {"repeats": REPEATS, "sub_batch": SUBBATCH, "seed_eval": CRN_SEED_EU, "outdir": OUT_DIR}

    # Ïã§Ìñâ
    run_common(
        train_fn=train_fn,
        rmse_fn=rmse_fn,
        seed_train=None,  # pgdpo_base ÎÇ¥Î∂ÄÏóêÏÑú user seed ÏÇ¨Ïö©
        train_kwargs=train_kwargs,
        rmse_kwargs=rmse_kwargs,
    )

    write_text(os.path.join(OUT_DIR, "_DONE"), "ok\n")




# ---------------------------------------------------------------------
# Ïò§ÏºÄÏä§Ìä∏Î†àÏù¥ÌÑ∞(Î≥ëÎ†¨ Ïã§Ìñâ) Î™®Îìú
# ---------------------------------------------------------------------

def _model_supports_residual(ROOT: str, model: str) -> bool:
    """
    tests/<model>/user_pgdpo_residual.py Ï°¥Ïû¨ Ïó¨Î∂Ä ÎòêÎäî
    user_pgdpo_base ÎÇ¥ residual Í¥ÄÎ†® Ïã¨Î≥º Ï°¥Ïû¨ Ïó¨Î∂ÄÎ°ú residual ÏßÄÏõê Í∞êÏßÄ.
    """
    import importlib
    test_dir = os.path.join(ROOT, "tests", model)
    has_residual_file = os.path.isfile(os.path.join(test_dir, "user_pgdpo_residual.py"))

    has_sim_residual = False
    try:
        sys.path.insert(0, test_dir)
        upb = importlib.import_module("user_pgdpo_base")
        has_sim_residual = any(
            hasattr(upb, name)
            for name in (
                "simulate_residual",
                "ResidualPolicy",
                "DirectPolicyResidual",
                "ResidualDirectPolicy",
            )
        )
    except Exception:
        pass
    finally:
        try:
            sys.path.remove(test_dir)
        except ValueError:
            pass

    return bool(has_residual_file or has_sim_residual)


def orchestrate_all(
    *,
    model: str,
    gpus: List[int],
    plots_base: str,
):
    """
    4Í∞ú Î™®Îìú(base/run/projection/residual)Î•º Î≥ëÎ†¨ Ïã§Ìñâ.
    Îã®, Î™®Îç∏Ïù¥ residualÏùÑ ÏßÄÏõêÌïòÏßÄ ÏïäÏúºÎ©¥ base/run/projection 3Í∞úÎßå Ïã§Ìñâ.
    """
    ROOT = os.path.dirname(os.path.abspath(__file__))
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    multi_dir = os.path.join(ROOT, plots_base, model, "multi", ts)
    ensure_dir(multi_dir)

    # Î™®Îç∏ Í∏∞Îä• Í∞êÏßÄ
    supports_residual = _model_supports_residual(ROOT, model)

    modes = ["base", "run", "projection"]
    if supports_residual:
        modes.append("residual")

    # GPU Îß§Ìïë Ï§ÄÎπÑ
    if len(gpus) == 0:
        if torch.cuda.is_available():
            gpus = list(range(torch.cuda.device_count()))
        else:
            gpus = [-1]  # CPU

    gpu_map = [gpus[i % len(gpus)] for i in range(len(modes))]

    # ÏöîÏïΩ Î©îÌÉÄ
    summary: Dict[str, Any] = {
        "model": model,
        "timestamp": ts,
        "orchestrator_dir": multi_dir,
        "modes": [],
        "git_commit": get_git_commit_or_none(),
        "torch_version": torch.__version__,
        "detected_support": {"residual": supports_residual},
    }

    procs = []
    log_files = []
    print("üß© Orchestrating multi-run:")
    for i, (mode_key, gpu_id) in enumerate(zip(modes, gpu_map)):
        # ÏûêÏãù Ïª§Îß®Îìú Íµ¨ÏÑ±
        cmd = [
            sys.executable,
            os.path.abspath(__file__),
            f"--{mode_key}", model,
            "--plots", plots_base,
            "--tag", f"auto-{ts}-{mode_key}"
        ]
        env = os.environ.copy()
        env["PYTHONUNBUFFERED"] = "1"
        if gpu_id >= 0 and torch.cuda.is_available():
            env["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
            cmd += ["--gpu", "0"]    # ÏûêÏãùÏùÄ ÏûêÏã†Ïùò 0Î≤àÏùÑ ÏÇ¨Ïö©
            gpu_label = f"cuda:{gpu_id}"
        else:
            env["CUDA_VISIBLE_DEVICES"] = ""
            cmd += ["--gpu", "-1"]
            gpu_label = "cpu"

        # Î°úÍ∑∏ ÌååÏùº
        log_path = os.path.join(multi_dir, f"{i:02d}_{mode_key}_gpu{gpu_id if gpu_id>=0 else 'cpu'}.log")
        lf = open(log_path, "w", encoding="utf-8")
        log_files.append(lf)

        print(f"  ‚Ä¢ [{mode_key}] ‚Üí GPU={gpu_label}  log={log_path}")
        p = subprocess.Popen(cmd, stdout=lf, stderr=lf, env=env, cwd=ROOT)
        procs.append((mode_key, gpu_id, p, log_path))

        summary["modes"].append({
            "mode": mode_key,
            "gpu": gpu_id,
            "log": log_path,
            "pid": p.pid,
            "cmd": cmd,
        })

    # ÏöîÏïΩ Î®ºÏ†Ä Í∏∞Î°ù
    write_json(os.path.join(multi_dir, "orchestrator.json"), summary)
    write_text(os.path.join(multi_dir, "README.txt"),
               "This folder contains logs for parallel runs started by --all.\n"
               "Each child writes its outputs under plots/<model>/<mode>/<timestamp>/.\n")

    # ÎåÄÍ∏∞
    retcodes = {}
    for mode_key, gpu_id, p, log_path in procs:
        rc = p.wait()
        retcodes[mode_key] = rc

    # Î°úÍ∑∏ Îã´Í∏∞
    for lf in log_files:
        try:
            lf.flush(); lf.close()
        except Exception:
            pass

    # Í≤∞Í≥º ÏöîÏïΩ Í∞±Ïã†
    summary["return_codes"] = retcodes
    write_json(os.path.join(multi_dir, "orchestrator_result.json"), summary)

    print("‚úÖ All runs finished.")
    for mode_key in modes:
        rc = retcodes.get(mode_key, None)
        print(f"  - {mode_key}: return_code={rc}")



# ---------------------------------------------------------------------
# ÏóîÌä∏Î¶¨
# ---------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description="PG-DPO runner")

    # Îã®Ïùº/Î≥ëÎ†¨ Î™®Îìú
    mode = parser.add_mutually_exclusive_group(required=True)
    mode.add_argument("--run", metavar="MODEL", help="Variance-reduced simulation mode")
    mode.add_argument("--projection", metavar="MODEL", help="P-PGDPO with projection mode")
    mode.add_argument("--base", metavar="MODEL", help="Basic simulation mode")
    mode.add_argument("--residual", metavar="MODEL", help="Residual learning mode")
    mode.add_argument("--all", metavar="MODEL", help="Run all supported modes concurrently for MODEL")

    parser.add_argument("--gpu", type=str, default=None, help="GPU index or CSV (e.g., 0 or 1,2,3,4,5). For --all, CSV maps to modes.")
    parser.add_argument("--plots", type=str, default="plots", help="Base plots directory")
    parser.add_argument("--tag", type=str, default=None, help="Optional tag appended to folder name")

    args = parser.parse_args()

    # Î≥ëÎ†¨(--all) Î™®Îìú
    if args.all:
        MODEL = args.all
        GPU_LIST = parse_gpu_list(args.gpu)
        orchestrate_all(model=MODEL, gpus=GPU_LIST, plots_base=args.plots)
        return

    # Îã®Ïùº Î™®Îìú
    if args.run:
        MODE = "run"
        MODEL = args.run
    elif args.projection:
        MODE = "projection"
        MODEL = args.projection
    elif args.base:
        MODE = "base"
        MODEL = args.base
    else:
        MODE = "residual"
        MODEL = args.residual

    # Îã®Ïùº Î™®Îìú GPU ÏÖÄÎ†âÌÑ∞ Íµ¨ÏÑ±
    # - --gpuÍ∞Ä "2"Î©¥ cuda:2
    # - --gpuÍ∞Ä "-1"Î©¥ cpu
    # - NoneÏù¥Î©¥ ÏûêÎèô
    gpu_sel = "auto"
    if args.gpu is not None:
        gstr = str(args.gpu).strip()
        if gstr == "-1":
            gpu_sel = "cpu"
        else:
            try:
                idx = int(gstr)
                gpu_sel = f"cuda:{idx}"
            except Exception:
                # CSVÍ∞Ä Îì§Ïñ¥ÏôîÏùÑ Ïàò ÏûàÏùå -> Ï≤´ Î≤àÏß∏Îßå ÏÇ¨Ïö©
                lst = parse_gpu_list(gstr)
                if len(lst) > 0:
                    gpu_sel = f"cuda:{lst[0]}"
                else:
                    gpu_sel = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        gpu_sel = "cuda" if torch.cuda.is_available() else "cpu"

    single_run(
        mode_key=MODE,
        model=MODEL,
        gpu_sel=gpu_sel,
        plots_base=args.plots,
        tag=args.tag,
    )


if __name__ == "__main__":
    main()